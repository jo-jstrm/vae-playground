{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Nice guide](https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchsummary einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, img_size: int=32, input_dim: int=3, hidden_dims: list=[32, 64], latent_dim: int=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Keep track of the spatial dims for the linear layer.\n",
    "        # This is required when working with the flattened output of the conv layers.\n",
    "        # For each conv layer, the spatial dims are halved.        \n",
    "        # Assumptions: square images, stride=2, padding=1.\n",
    "        self.spatial_dims_inner_conv = img_size // (2 ** len(hidden_dims))\n",
    "        print(f'Linear spatial dims: {self.spatial_dims_inner_conv}')\n",
    "\n",
    "        encoder_layers = []\n",
    "        in_channels = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            encoder_layers.append(\n",
    "                torch.nn.Conv2d(in_channels, h_dim, kernel_size=3, stride=2, padding=1))\n",
    "            encoder_layers.append(torch.nn.ReLU())\n",
    "            in_channels = h_dim        \n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)            \n",
    "\n",
    "        # The input size is the number of channels of the last conv layer \n",
    "        # times the spatial dims squared.\n",
    "        self.linear_mu = torch.nn.Linear(self.hidden_dims[-1] * self.spatial_dims_inner_conv**2,\n",
    "                                         self.latent_dim)\n",
    "        self.linear_std= torch.nn.Linear(self.hidden_dims[-1] * self.spatial_dims_inner_conv**2,\n",
    "                                         self.latent_dim)\n",
    "            \n",
    "\n",
    "        decoder_layers = []\n",
    "        in_channels = hidden_dims[-1]\n",
    "        reversed_hidden_dims = list(reversed(hidden_dims))\n",
    "        for i in range(len(reversed_hidden_dims) - 1):\n",
    "            decoder_layers.append(\n",
    "                torch.nn.ConvTranspose2d(reversed_hidden_dims[i], \n",
    "                                         reversed_hidden_dims[i+1],\n",
    "                                         kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(torch.nn.ReLU())        \n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers, \n",
    "                                           torch.nn.ConvTranspose2d(reversed_hidden_dims[-1], \n",
    "                                                                    input_dim,\n",
    "                                                                    kernel_size=3, \n",
    "                                                                    stride=2, \n",
    "                                                                    padding=1,\n",
    "                                                                    output_padding=1),\n",
    "                                           torch.nn.Tanh())\n",
    "        self.linear_decoder = torch.nn.Linear(latent_dim, \n",
    "                                              hidden_dims[-1] * self.spatial_dims_inner_conv**2)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        # Keep the batch dim unchanged.\n",
    "        x = einops.rearrange(x, 'b c h w -> b (c h w)')\n",
    "        mus = self.linear_mu(x)\n",
    "        log_vars = self.linear_std(x)\n",
    "        return mus, log_vars\n",
    "    \n",
    "    def draw_sample(self, mus: torch.Tensor, log_vars: torch.Tensor):\n",
    "        eps = torch.randn_like(log_vars)\n",
    "        # Reparameterization trick\n",
    "        # Any normal distribution can be constructed by using \n",
    "        # a standard normal distribution (epsilon), scaling it \n",
    "        # by the standard deviation (sigma) and then shifting by the mean (mu).\n",
    "        z = mus + log_vars * eps\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_hat = self.linear_decoder(z)\n",
    "        x_hat = einops.rearrange(x_hat, 'b (c h w) -> b c h w',\n",
    "                                 c=self.hidden_dims[-1],\n",
    "                                 h=self.spatial_dims_inner_conv,\n",
    "                                 w=self.spatial_dims_inner_conv)\n",
    "        x_hat = self.decoder(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Mu and sigma represent the parameters of n = mu.shape[-1] normal distributions.\n",
    "        mus, log_vars = self.encode(x)\n",
    "        # Of these n normal distributions, we draw n samples.\n",
    "        z = self.draw_sample(mus, log_vars)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mus, log_vars\n",
    "\n",
    "def elbo_loss(\n",
    "        x: torch.Tensor, x_hat: torch.Tensor, mus: torch.Tensor, log_var: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Calculates the ELBO loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Z-score normalized input images.\n",
    "    x_hat : torch.Tensor\n",
    "        Reconstruction of the input images.\n",
    "    mus : torch.Tensor\n",
    "        Mu values of the latent space.\n",
    "    log_vars : torch.Tensor\n",
    "        Sigma values of the latent space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor        \n",
    "    \"\"\"\n",
    "    # Rescale the output to [0, 1] to be able to use the MSE loss.\n",
    "    x_hat = (1 + x_hat) / 2\n",
    "    mse = F.mse_loss(x_hat, x, reduction='none')\n",
    "    # We want the distribution of the latent space to be as close as possible to a standard normal distribution.    \n",
    "    # Taken from https://github.com/AntixK/PyTorch-VAE/blob/a6896b944c918dd7030e7d795a8c13e5c6345ec7/models/vanilla_vae.py#L143C105-L143C105\n",
    "    # KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "    # Derivation: https://github.com/AntixK/PyTorch-VAE/issues/69\n",
    "    # The derived formula using the log_var is better, because it allows for the values to be \n",
    "    # negative during training. When treating them as Sigma, they would have to be positive,\n",
    "    # which is hard to enforce.\n",
    "    d_kl = torch.mean(-0.5 * torch.sum(1 + log_var - mus ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "    beta = 1\n",
    "    elbo = mse + beta * d_kl\n",
    "    return elbo.mean()\n",
    "\n",
    "# from torchsummary import summary\n",
    "# print(vae)\n",
    "# summary(vae, (3, 32, 32), batch_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear spatial dims: 8\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as T\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module):\n",
    "    num_epochs = 1000\n",
    "    batch_size = 512\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    dataset = CIFAR10(root='./data', download=True, train=True, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    with tqdm(desc=f'Training...', total=num_epochs) as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (x, _) in enumerate(dataloader):\n",
    "                x = x.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_hat, mus, log_vars = model(x)\n",
    "                loss = elbo_loss(x, x_hat, mus, log_vars)\n",
    "                loss.backward()\n",
    "                optimizer.step()                \n",
    "            pbar.set_postfix({'loss': f'{loss.item():0.6f}'})\n",
    "            pbar.update(1)\n",
    "    torch.save(model.cpu().state_dict(), f'data/checkpoints/{datetime.now()}_vae_{epoch}.pt')\n",
    "\n",
    "vae = VAE().cuda()\n",
    "train(vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
