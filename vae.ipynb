{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for Images\n",
    "\n",
    "- Small CNN-based network\n",
    "- Training on CIFAR10\n",
    "- Logging via tensorboard\n",
    "- [Nice guide](https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torcheval torchsummary tensorboard einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, img_size: int=32, input_dim: int=3, hidden_dims: list=[32, 64], latent_dim: int=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Keep track of the spatial dims for the linear layer.\n",
    "        # This is required when working with the flattened output of the conv layers.\n",
    "        # For each conv layer, the spatial dims are halved.        \n",
    "        # Assumptions: square images, stride=2, padding=1.\n",
    "        self.spatial_dims_inner_conv = img_size // (2 ** len(hidden_dims))\n",
    "        print(f'Linear spatial dims: {self.spatial_dims_inner_conv}')\n",
    "\n",
    "        encoder_layers = []\n",
    "        in_channels = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            encoder_layers.append(\n",
    "                torch.nn.Conv2d(in_channels, h_dim, kernel_size=3, stride=2, padding=1))\n",
    "            encoder_layers.append(torch.nn.ReLU())\n",
    "            in_channels = h_dim        \n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)            \n",
    "\n",
    "        # The input size is the number of channels of the last conv layer \n",
    "        # times the spatial dims squared.\n",
    "        self.linear_mu = torch.nn.Linear(self.hidden_dims[-1] * self.spatial_dims_inner_conv**2,\n",
    "                                         self.latent_dim)\n",
    "        self.linear_std= torch.nn.Linear(self.hidden_dims[-1] * self.spatial_dims_inner_conv**2,\n",
    "                                         self.latent_dim)\n",
    "            \n",
    "\n",
    "        decoder_layers = []\n",
    "        in_channels = hidden_dims[-1]\n",
    "        reversed_hidden_dims = list(reversed(hidden_dims))\n",
    "        for i in range(len(reversed_hidden_dims) - 1):\n",
    "            decoder_layers.append(\n",
    "                torch.nn.ConvTranspose2d(reversed_hidden_dims[i], \n",
    "                                         reversed_hidden_dims[i+1],\n",
    "                                         kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            decoder_layers.append(torch.nn.ReLU())        \n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers, \n",
    "                                           torch.nn.ConvTranspose2d(reversed_hidden_dims[-1], \n",
    "                                                                    input_dim,\n",
    "                                                                    kernel_size=3, \n",
    "                                                                    stride=2, \n",
    "                                                                    padding=1,\n",
    "                                                                    output_padding=1),\n",
    "                                           torch.nn.Tanh())\n",
    "        self.linear_decoder = torch.nn.Linear(latent_dim, \n",
    "                                              hidden_dims[-1] * self.spatial_dims_inner_conv**2)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        # Keep the batch dim unchanged.\n",
    "        x = einops.rearrange(x, 'b c h w -> b (c h w)')\n",
    "        mus = self.linear_mu(x)\n",
    "        log_vars = self.linear_std(x)\n",
    "        return mus, log_vars\n",
    "    \n",
    "    def draw_sample(self, mus: torch.Tensor, log_vars: torch.Tensor):\n",
    "        eps = torch.randn_like(log_vars)\n",
    "        # Reparameterization trick\n",
    "        # Any normal distribution can be constructed by using \n",
    "        # a standard normal distribution (epsilon), scaling it \n",
    "        # by the standard deviation (sigma) and then shifting by the mean (mu).\n",
    "        z = mus + log_vars * eps\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_hat = self.linear_decoder(z)\n",
    "        x_hat = einops.rearrange(x_hat, 'b (c h w) -> b c h w',\n",
    "                                 c=self.hidden_dims[-1],\n",
    "                                 h=self.spatial_dims_inner_conv,\n",
    "                                 w=self.spatial_dims_inner_conv)\n",
    "        x_hat = self.decoder(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Mu and sigma represent the parameters of n = mu.shape[-1] normal distributions.\n",
    "        mus, log_vars = self.encode(x)\n",
    "        # Of these n normal distributions, we draw n samples.\n",
    "        z = self.draw_sample(mus, log_vars)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mus, log_vars\n",
    "\n",
    "def elbo_loss(\n",
    "        x: torch.Tensor, x_hat: torch.Tensor, mus: torch.Tensor, log_var: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Calculates the ELBO loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Z-score normalized input images.\n",
    "    x_hat : torch.Tensor\n",
    "        Reconstruction of the input images.\n",
    "    mus : torch.Tensor\n",
    "        Mu values of the latent space.\n",
    "    log_vars : torch.Tensor\n",
    "        Sigma values of the latent space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor        \n",
    "    \"\"\"\n",
    "    # Rescale the output to [0, 1] to be able to use the MSE loss.\n",
    "    x_hat = (1 + x_hat) / 2\n",
    "    mse = F.mse_loss(x_hat, x, reduction='none')\n",
    "    # We want the distribution of the latent space to be as close as possible to a standard normal distribution.    \n",
    "    # Taken from https://github.com/AntixK/PyTorch-VAE/blob/a6896b944c918dd7030e7d795a8c13e5c6345ec7/models/vanilla_vae.py#L143C105-L143C105\n",
    "    # KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "    # Derivation: https://github.com/AntixK/PyTorch-VAE/issues/69\n",
    "    # The derived formula using the log_var is better, because it allows for the values to be \n",
    "    # negative during training. When treating them as Sigma, they would have to be positive,\n",
    "    # which is hard to enforce.\n",
    "    d_kl = torch.mean(-0.5 * torch.sum(1 + log_var - mus ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "    beta = 1\n",
    "    elbo = mse + beta * d_kl\n",
    "    return elbo.mean()\n",
    "\n",
    "# from torchsummary import summary\n",
    "# print(vae)\n",
    "# summary(vae, (3, 32, 32), batch_size=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics.functional import peak_signal_noise_ratio\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def validate(model: torch.nn.Module,\n",
    "             epoch: int,\n",
    "             log_writer: SummaryWriter,\n",
    "             dataloader: torch.utils.data.DataLoader,\n",
    "             test: bool=False\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        elbo_losses = []\n",
    "        psnr_scores = []\n",
    "        mse_scores = []\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.cuda()\n",
    "            x_hat, mus, log_vars = model(x)\n",
    "            elbo_losses.append(elbo_loss(x, x_hat, mus, log_vars))\n",
    "            psnr_scores.append(peak_signal_noise_ratio(x_hat, x))\n",
    "            mse_scores.append(F.mse_loss(x_hat, x))\n",
    "            if i == 0:\n",
    "                x_grid = torchvision.utils.make_grid(\n",
    "                                x[:8].unsqueeze(0), nrow=2, normalize=True, scale_each=True)\n",
    "                x_hat_grid = torchvision.utils.make_grid(\n",
    "                                x_hat[:8].unsqueeze(0), nrow=2, normalize=True, scale_each=True)\n",
    "                log_writer.add_images('Input Images', x_grid, epoch)\n",
    "                log_writer.add_images('Reconstructed Images', x_hat_grid, epoch)\n",
    "        avg_elbo = torch.stack(elbo_losses).mean()\n",
    "        avg_mse_score = torch.stack(mse_scores).mean()\n",
    "        avg_psnr_score = torch.stack(psnr_scores).mean()\n",
    "        # We have the same loop for validation and testing. Adapt the log names accordingly.\n",
    "        run_type = 'Test' if test else 'Validation'        \n",
    "        log_writer.add_scalar(f'{run_type} ELBO', avg_elbo, epoch)\n",
    "        log_writer.add_scalar(f'{run_type} MSE', avg_mse_score, epoch)\n",
    "        log_writer.add_scalar(f'{run_type} PSNR', avg_psnr_score, epoch)\n",
    "        # print(f'    {run_type} metrics, averaged over validation set: '\n",
    "        #         f'MSE: {avg_mse_score:0.6f}, '\n",
    "        #         f'PSNR: {avg_psnr_score:0.6f}')\n",
    "\n",
    "\n",
    "def test(\n",
    "        model: torch.nn.Module, log_writer: SummaryWriter, batch_size: int=512, num_workers: int=2\n",
    "):\n",
    "    log_writer = SummaryWriter()\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    test_set = CIFAR10(root='./data', download=False, train=False, transform=transform)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "                    test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    validate(model, 0, log_writer, test_dataloader, test=True)\n",
    "    print('Testing Done.')\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          log_writer: SummaryWriter,\n",
    "          checkpoint_dir: str,\n",
    "          pretrained_path: str=None,\n",
    "          num_epochs: int=1000,\n",
    "          batch_size: int=512,\n",
    "          num_workers: int=2,\n",
    "          val_freq: int=100,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Train the model, log metrics and save checkpoints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Model to train.\n",
    "    log_writer : SummaryWriter\n",
    "        Tensorboard logger.\n",
    "    checkpoint_dir: str\n",
    "        Directory to save checkpoints to.\n",
    "    pretrained_path: str, optional\n",
    "        Path to a pre-trained checkpoint, by default None\n",
    "    num_epochs : int, optional\n",
    "        Number of epochs to train. If there is a pretrained checkpoint given,\n",
    "        these epochs are added on top of the checkpoint's epochs. By default 1000.\n",
    "    batch_size : int, optional\n",
    "        by default 512\n",
    "    num_workers : int, optional\n",
    "        by default 2\n",
    "    val_freq : int, optional\n",
    "        Validate every *val_freq* epochs., by default 100\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "    checkpoint_freq = 1000\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    train_set = CIFAR10(root='./data', download=True, train=True, transform=transform)\n",
    "    val_set = CIFAR10(root='./data', download=False, train=True, transform=transform)\n",
    "    split = int(len(train_set) * 0.8)\n",
    "    train_set.data = train_set.data[:split]\n",
    "    val_set.data = val_set.data[split:]\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "                    train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "                    val_set,batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    pre_trained_epoch = 0\n",
    "    if pretrained_path:\n",
    "        model.load_state_dict(torch.load(pretrained_path))\n",
    "        pre_trained_epoch = int(Path(pretrained_path).stem.split('_')[-1]) + 1\n",
    "    with tqdm(desc=f'Training...',\n",
    "              total=pre_trained_epoch + num_epochs,\n",
    "              initial=pre_trained_epoch\n",
    "    ) as pbar:\n",
    "        for epoch in range(pre_trained_epoch, pre_trained_epoch + num_epochs):\n",
    "            model.train()\n",
    "            epoch_losses = []\n",
    "            for i, (x, _) in enumerate(train_dataloader):\n",
    "                x = x.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                x_hat, mus, log_vars = model(x)\n",
    "                loss = elbo_loss(x, x_hat, mus, log_vars)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_losses.append(loss)\n",
    "            log_writer.add_scalar('Train ELBO loss', torch.stack(epoch_losses).mean(), epoch)\n",
    "            if epoch % 10 == 0:\n",
    "                pbar.set_postfix({'loss': f'{loss.item():0.6f}'})\n",
    "            if epoch % val_freq == 0:\n",
    "                validate(model, epoch, log_writer, val_dataloader)\n",
    "            if epoch % checkpoint_freq == 0:\n",
    "                save_path = Path(checkpoint_dir) / f'{datetime.now()}_vae_{epoch}.pt'\n",
    "                torch.save(model.state_dict(), save_path)                            \n",
    "            pbar.update(1)\n",
    "    log_writer.flush()\n",
    "    log_writer.close()\n",
    "    save_path = Path(checkpoint_dir) / f'{datetime.now()}_vae_{epoch}.pt'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print('Training Done.')\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def run_training(pretrained_file: str=None,\n",
    "                 checkpoint_dir: str='/home/jo/git/vae-playground/data/checkpoints/'\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    log_writer = SummaryWriter()\n",
    "    vae = VAE().to(device)    \n",
    "    pretrained_path = checkpoint_dir + pretrained_file if pretrained_file else None\n",
    "    # train(vae, num_epochs=2, val_freq=1)\n",
    "    vae = train(vae, \n",
    "                log_writer=log_writer,\n",
    "                checkpoint_dir=checkpoint_dir,\n",
    "                pretrained_path=pretrained_path,\n",
    "                num_epochs=10000,\n",
    "                val_freq=100)\n",
    "    test(vae, log_writer)\n",
    "\n",
    "\n",
    "run_training(pretrained_file='2023-09-06 11:42:33.149736_vae_1999.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
